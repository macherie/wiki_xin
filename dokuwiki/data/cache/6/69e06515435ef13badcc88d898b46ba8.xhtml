<pre class="code">                                                扫描头尺寸转换的分析
                                                            --胡建</pre>

<p>
 转换提示: 读下面的文字时，希望大家结合图片看，这样更易理解
 在YUV420中，一个像素点对应一个Y，一个2&times;2的小方块对应一个U和V。对于所有YUV420图像，它们的Y值排列是完全相同的，因为只有Y的图像就是灰度图像。
 YUV420sp与YUV420p的数据格式它们的UV排列在原理上是完全不同的。420p它是先把U存放完后，再存放V，也就是说UV它们是连续的。而420sp它是UV、UV这样交替存放的。(见下图)
 有了上面的理论，我就可以准确的计算出一个YUV420在内存中存放的大小。
width * hight =Y（总和）
U = Y / 4  
V = Y / 4
</p>

<p>
所以YUV420 数据在内存中的长度是 width * hight * 3 / 2，
</p>

<p>
假设一个分辨率为8&times;4的YUV图像，它们的格式如下图：
</p>
<pre class="code">   YUV420sp格式如下图            YUV420p数据格式如下图</pre>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;1346422959_6364.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:1346422959_6364.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;1346422959_6364.png?w=200&amp;tok=c8a82a" class="media" alt="" width="200" /></a>   <a href="/dokuwiki/lib/exe/detail.php/android;camera;1346422970_2927.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:1346422970_2927.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;1346422970_2927.png?w=200&amp;tok=58b586" class="media" alt="" width="200" /></a>
</p>

<p>
我们在扫描头在解码的时候就遇到了一个类似的难题，在nodisplay_preview_stream_cb_routine函数处理过程中,含有meta数据的bufsize与解码需要的尺寸存在不匹配，导致解码会超时
后来通过算法解决了这个难题，这个算法就用到了上面的理论：
以下是算法具体内容：
</p>
<pre class="code">  camera_memory_t *data = NULL;
  size_t previewBufSize = 0;
  size_t previewBufSizeFromCallback = 0;
  cam_dimension_t preview_dim;
  cam_format_t previewFmt;
  int32_t rc = NO_ERROR;
  int32_t yStride = 0;
  int32_t yScanline = 0;
  int32_t uvStride = 0;
  int32_t uvScanline = 0;
  int32_t uStride = 0;
  int32_t uScanline = 0;
  int32_t vStride = 0;
  int32_t vScanline = 0;
  int32_t yStrideToApp = 0;
  int32_t uvStrideToApp = 0;
  int32_t yScanlineToApp = 0;
  int32_t uvScanlineToApp = 0;
  int32_t srcOffset = 0;
  int32_t dstOffset = 0;
  int32_t srcBaseOffset = 0;
  int32_t dstBaseOffset = 0;
  int i;</pre>

<p>
通过superframe 获取preview的memory
</p>
<pre class="code">  QCameraGrallocMemory *memory = (QCameraGrallocMemory *)super_frame-&gt;bufs[0]-&gt;mem_info;
  uint32_t idx = frame-&gt;buf_idx;
  if ((NULL == stream) || (NULL == memory)) {
ALOGE(&quot;%s: Invalid preview callback input&quot;, __func__);
return ;
  }
  cam_stream_info_t *streamInfo =
          reinterpret_cast&lt;cam_stream_info_t *&gt;(stream-&gt;getStreamInfoBuf()-&gt;getPtr(0));
  if (NULL == streamInfo) {
      ALOGE(&quot;%s: Invalid streamInfo&quot;, __func__);
      return ;
  }</pre>

<p>
通过streaminfo获取y,uv分量的值和previewBufSize 和callback的值
</p>
<pre class="code">  stream-&gt;getFrameDimension(preview_dim);
  stream-&gt;getFormat(previewFmt);
  yStride = streamInfo-&gt;buf_planes.plane_info.mp[0].stride;
  yScanline = streamInfo-&gt;buf_planes.plane_info.mp[0].scanline;
  uvStride = streamInfo-&gt;buf_planes.plane_info.mp[1].stride;
  uvScanline = streamInfo-&gt;buf_planes.plane_info.mp[1].scanline;
  yStrideToApp = preview_dim.width;
  yScanlineToApp = preview_dim.height;
  uvStrideToApp = yStrideToApp;
  uvScanlineToApp = yScanlineToApp / 2;
  previewBufSize = (size_t)
          ((yStrideToApp * yScanlineToApp) + (uvStrideToApp * uvScanlineToApp));
  previewBufSizeFromCallback = (size_t)
          ((yStride * yScanline) + (uvStride * uvScanline));
  data = memory-&gt;getMemory(idx, false);
  if (!preview_mem || !preview_mem-&gt;data) {
      ALOGE(&quot;%s: mGetMemory failed.\n&quot;, __func__);
      return ;
  }</pre>

<p>
这个是核心算法，用来移除多余的height分量，此时转换的尺寸是768*480 –» 752*480
</p>
<pre class="code">  for (i = 0; i &lt; preview_dim.height; i++) {
      srcOffset = i * yStride;
      dstOffset = i * yStrideToApp;
      memcpy((unsigned char *) preview_mem-&gt;data + dstOffset,
              (unsigned char *) data-&gt;data + srcOffset, (size_t) yStrideToApp);
  }
  srcBaseOffset = yStride * yScanline;
  dstBaseOffset = yStrideToApp * yScanlineToApp;
  for (i = 0; i &lt; preview_dim.height/2; i++) {
      srcOffset = i * uvStride + srcBaseOffset;
      dstOffset = i * uvStrideToApp + dstBaseOffset;
      memcpy((unsigned char *) preview_mem-&gt;data + dstOffset,
              (unsigned char *) data-&gt;data + srcOffset,
              (size_t) yStrideToApp);
  }</pre>

<p>
传给CBdata处理
</p>
<pre class="code">      if (pme-&gt;needProcessPreviewFrame() &amp;&amp;
          pme-&gt;mDataCb != NULL &amp;&amp;
          pme-&gt;msgTypeEnabledWithLock(CAMERA_MSG_PREVIEW_FRAME) &gt; 0 ) {
          qcamera_callback_argm_t cbArg;
          memset(&amp;cbArg, 0, sizeof(qcamera_callback_argm_t));
          cbArg.cb_type = QCAMERA_DATA_CALLBACK;
          cbArg.msg_type = CAMERA_MSG_PREVIEW_FRAME;
          cbArg.data = preview_mem;
          cbArg.user_data = (void *) &amp;frame-&gt;buf_idx;
          cbArg.cookie = stream;
          cbArg.release_cb = returnStreamBuffer;
          int32_t rc = pme-&gt;m_cbNotifier.notifyCallback(cbArg);
          if (rc != NO_ERROR) {
              ALOGE(&quot;%s: fail sending data notify&quot;, __func__);
              stream-&gt;bufDone(frame-&gt;buf_idx);
          }  }</pre>

<p>
RGB Bayer Color分析
</p>

<p>
拜耳色彩滤波阵列（Bayer Color Filter Array，CFA）是非常有名的彩色图片的数字采集格式。色彩滤波器的模式如上图所示，由一半的G，1/4的R，1/4的B组成。
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;aa.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:aa.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;aa.png?w=200&amp;tok=295616" class="media" alt="" width="200" /></a>
</p>

<p>
拜耳色彩滤波器的模式、序列、滤波器有很多种，但最常见的模式是由Kodak提出的2*2模式。
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;bb.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:bb.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;bb.png?w=200&amp;tok=c62429" class="media" alt="" width="200" /></a>
</p>

<p>
当Image Sensor往外逐行输出数据时，像素的序列为GRGRGR…/BGBGBG…（顺序RGB）。这样阵列的Sensor设计，使得RGB传感器减少到了全色传感器的1/3，如下所示。
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;cc.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:cc.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;cc.png?w=200&amp;tok=a8dccb" class="media" alt="" width="200" /></a>
</p>

<p>
图像传感器的结构如下所示，每一个感光像素之间都有金属隔离层，光纤通过显微镜头，在色彩滤波器过滤之后，投射到相应的漏洞式硅的感光元件上。
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;dd.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:dd.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;dd.png?w=200&amp;tok=0cfc35" class="media" alt="" width="200" /></a>
</p>

<p>
白平衡调节（White Balance）
</p>

<p>
色彩传感器并不能像人眼那样直接感应图像，因此为了保证最终图像的真实性，必须经过一些白平衡处理以及色彩校正等算法来修正图像。
</p>

<p>
原始像素的第一步处理操作就是白平衡调节。一个白色物体每通道的白平衡都应该是相同的，即R=G=B。通过白色物体的采集以及直方图分析，拥有最高级别白平衡的通道被作为目标通道，而其他两个通道通过增益达到匹配，如下：R&#039;=aG&#039;=bB&#039;。同时，随着光源的不同，白平衡也应该相应的调节。
Bayer插值补偿算法（Bayer Interpolation）
</p>

<p>
1) 插值红蓝算法实现
</p>

<p>
每一个像素仅仅包括了光谱的一部分，必须通过插值来实现每个像素的RGB值。为了从Bayer格式得到每个像素的RGB格式，我们需要通过插值填补缺失的2个色彩。插值的方法有很多（包括领域、线性、3*3等），速度与质量权衡，最好的线性插值补偿算法。其中算法如下：
</p>

<p>
R和B通过线性领域插值，但这有四种不同的分布，如下图所示：
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;ee.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:ee.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;ee.png?w=200&amp;tok=638613" class="media" alt="" width="200" /></a>
</p>

<p>
（a）                                （b）
</p>

<p>
在（a）与（b）中，R和B分别取领域的平均值。
</p>

<p>
2) 插值绿算法实现
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;ff.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:ff.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;ff.png?w=200&amp;tok=068c07" class="media" alt="" width="200" /></a>
</p>

<p>
（c）                                （d）
</p>

<p>
在（c）与（d）中，取领域的4个B或R的均值作为中间像素的B值。
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;gg.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:gg.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;gg.png?w=200&amp;tok=05ff86" class="media" alt="" width="200" /></a>
</p>

<p>
由于人眼对绿光反应最敏感，对紫光和红光则反应较弱，因此为了达到更好的画质，需要对G特殊照顾。在上述（c）与（d）中，扩展开来就是上图的（e）与（f）中间像素G的取值，者也有一定的算法要求，不同的算法效果上会有差异。经过相关的研究，（e）中间像素G值的算法如下：
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;hh.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:hh.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;hh.png?w=200&amp;tok=21ea63" class="media" alt="" width="200" /></a>
</p>

<p>
 （e）             
</p>

<p>
（f）中间像素G值的算法如下：
 （f）
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;jj.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:jj.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;jj.png?w=200&amp;tok=7fd479" class="media" alt="" width="200" /></a>
</p>

<p>
CMOS摄像头这部分转换是在内部用ADC或者ISP完成的，生产商为了降低成本必然会使得图像失真。当然用外部处理器来实现转换，如果处理器的速度足够NB，能够胜任像素的操作，用上面的算法来进行转换，皆大欢喜。不过上述算法将直接成倍提高了算法的复杂度，速度上将会有所限制。因此为了速度的提成，可以直接通过来4领域G取均值来中间像素的G值，将会降低一倍的速率，而在性能上差之甚微，算法如下：
</p>

<p>
<a href="/dokuwiki/lib/exe/detail.php/android;camera;kk.png?id=android%3Acamera%3Ascanner_yuv_size_and_bayer" class="media" title="android:camera:kk.png"><img src="/dokuwiki/lib/exe/fetch.php/android;camera;kk.png?w=200&amp;tok=141c1f" class="media" alt="" width="200" /></a>
</p>

<p>
如果能够通过损失图像的额质量，来达到更快的速度，还可以取G1、G2的均值来实现，但是这样的做法会导致边沿以及跳变部分的失真。
</p>
